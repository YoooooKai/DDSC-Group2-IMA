{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79c3a19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Response Code: 200\n",
      "{\n",
      "    \"data\": [\n",
      "        {\n",
      "            \"created_at\": \"2012-05-10T14:38:56.000Z\",\n",
      "            \"description\": \"At the forefront of machine tool technology since 1983, supplying a unique range of intelligent precision engineering technologies throughout the UK\",\n",
      "            \"id\": \"576287914\",\n",
      "            \"name\": \"Whitehouse\",\n",
      "            \"public_metrics\": {\n",
      "                \"followers_count\": 1057,\n",
      "                \"following_count\": 376,\n",
      "                \"listed_count\": 9,\n",
      "                \"tweet_count\": 1418\n",
      "            },\n",
      "            \"url\": \"http://t.co/UKZNKshraL\",\n",
      "            \"username\": \"wmtcnc\",\n",
      "            \"verified\": false\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# The codes are adapted from the sample codes given by Twitterdev through GitHub repository: https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/main/User-Lookup/get_users_with_bearer_token.py\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "# run the belowing commented code the first time\n",
    "os.environ['TOKEN'] = 'AAAAAAAAAAThe AAAAAAAAAAAMfScAEAAAAALQJjppcjpBs3%2B06Sbn%2BcdAlA%2BCQ%3D52jIy4Vtch5lfjsqN2C46rwcotcVZthA6UKKJ7p24RMvHggh5y'\n",
    "\n",
    "bearer_token = os.environ.get(\"TOKEN\")\n",
    "\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2UserLookupPython\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def create_url():\n",
    "    # Specify the usernames that you want to lookup below\n",
    "    # You can enter up to 100 comma-separated values.\n",
    "    usernames = \"usernames=wmtcnc\" # Here is just example, put the name of the account that you want to search\n",
    "    user_fields = \"user.fields=description,created_at,id,public_metrics,url,verified\"\n",
    "    # User fields are adjustable, options include:\n",
    "    # created_at, description, entities, id, location, name,\n",
    "    # pinned_tweet_id, profile_image_url, protected,\n",
    "    # public_metrics, url, username, verified, and withheld\n",
    "    url = \"https://api.twitter.com/2/users/by?{}&{}\".format(usernames, user_fields)\n",
    "    return url\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url):\n",
    "    response = requests.request(\"GET\", url, auth=bearer_oauth)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def main():\n",
    "    url = create_url()\n",
    "    json_response = connect_to_endpoint(url)\n",
    "    print(json.dumps(json_response, indent=4, sort_keys=True))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad673366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Response Code: 200\n",
      "{\n",
      "    \"data\": [\n",
      "        {\n",
      "            \"author_id\": \"576287914\",\n",
      "            \"conversation_id\": \"1521444600858222592\",\n",
      "            \"created_at\": \"2022-05-03T11:00:34.000Z\",\n",
      "            \"id\": \"1521444600858222592\",\n",
      "            \"lang\": \"en\",\n",
      "            \"public_metrics\": {\n",
      "                \"like_count\": 1,\n",
      "                \"quote_count\": 0,\n",
      "                \"reply_count\": 0,\n",
      "                \"retweet_count\": 0\n",
      "            },\n",
      "            \"reply_settings\": \"everyone\",\n",
      "            \"source\": \"Twitter for iPhone\",\n",
      "            \"text\": \"Latest #BROTHER high speed VMCs under preparation today at @wmtcnc https://t.co/wheSDccHOB\"\n",
      "        },\n",
      "        {\n",
      "            \"author_id\": \"576287914\",\n",
      "            \"conversation_id\": \"1520719728712556544\",\n",
      "            \"created_at\": \"2022-05-01T11:00:11.000Z\",\n",
      "            \"id\": \"1520719728712556544\",\n",
      "            \"lang\": \"und\",\n",
      "            \"public_metrics\": {\n",
      "                \"like_count\": 0,\n",
      "                \"quote_count\": 0,\n",
      "                \"reply_count\": 0,\n",
      "                \"retweet_count\": 0\n",
      "            },\n",
      "            \"reply_settings\": \"everyone\",\n",
      "            \"source\": \"Twitter for iPad\",\n",
      "            \"text\": \"https://t.co/AGZjwzAEPR\"\n",
      "        }\n",
      "    ],\n",
      "    \"includes\": {\n",
      "        \"users\": [\n",
      "            {\n",
      "                \"created_at\": \"2012-05-10T14:38:56.000Z\",\n",
      "                \"description\": \"At the forefront of machine tool technology since 1983, supplying a unique range of intelligent precision engineering technologies throughout the UK\",\n",
      "                \"id\": \"576287914\",\n",
      "                \"name\": \"Whitehouse\",\n",
      "                \"public_metrics\": {\n",
      "                    \"followers_count\": 1057,\n",
      "                    \"following_count\": 376,\n",
      "                    \"listed_count\": 9,\n",
      "                    \"tweet_count\": 1418\n",
      "                },\n",
      "                \"username\": \"wmtcnc\",\n",
      "                \"verified\": false\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"meta\": {\n",
      "        \"newest_id\": \"1521444600858222592\",\n",
      "        \"oldest_id\": \"1520719728712556544\",\n",
      "        \"result_count\": 2\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# The codes for connecting to Twitter API are adapted from the sample codes given by Twitterdev through GitHub repository: https://github.com/twitterdev/Twitter-API-v2-sample-code/tree/main/User-Tweet-Timeline\n",
    "# This code is used to retrieve all the tweets published by a specific Twitter account. For more information, please check: https://developer.twitter.com/en/docs/twitter-api/tweets/timelines/introduction\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Run the belowing commented code the first time you run this code\n",
    "os.environ['TOKEN'] = 'AAAAAAAAAAAAAAAAAAAAAMfScAEAAAAALQJjppcjpBs3%2B06Sbn%2BcdAlA%2BCQ%3D52jIy4Vtch5lfjsqN2C46rwcotcVZthA6UKKJ7p24RMvHggh5y'\n",
    "bearer_token = os.environ.get(\"TOKEN\")\n",
    "\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2UserTweetsPython\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def create_url(start_time, end_time, max_results): # If you are retrieving recent tweets, remove \"start_date\" and \"end_date\"\n",
    "\t# crete url and set up the query parameters\n",
    "    user_id = 576287914 # you can get the user_id through Get_user.py\n",
    "   \n",
    "    search_url = \"https://api.twitter.com/2/users/{}/tweets?exclude=replies\".format(user_id) \n",
    "    # I have exluded the replies. If you want to include the replies, then you need to remove \"?exclude=replies\" from the url\n",
    "\n",
    "    #change params based on the endpoint you are using and what you need\n",
    "    query_params = {'start_time':start_time,\n",
    "                    'end_time': end_time,\n",
    "                    'max_results': max_results,\n",
    "                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function; next_token is the unique ID field for the next page of results\n",
    "    response = requests.request(\"GET\", url, auth=bearer_oauth, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "max_results = 50 #This defaults to 10 Tweets and has a maximum of 100. \n",
    "\n",
    "# You can specify start_time and end_time to get tweets within a certain period of time. If you don't specify the timeframe, you will get the most recent tweets. Based on the documentation, you can get up to 3,200 most recent Tweets, Retweets, replies and Quote Tweets posted by the user.\n",
    "start_time = '2022-04-27T00:00:00.000Z'\n",
    "end_time = '2022-05-04T00:00:00.000Z'\n",
    "url = create_url(start_time, end_time, max_results)\n",
    "json_response = connect_to_endpoint(url[0], url[1])\n",
    "print(json.dumps(json_response, indent=4, sort_keys=True))\n",
    "\n",
    "# Here will return 10 tweets within specified time period by the account (BMW twitter account, 1545994664) in json format.\n",
    "\n",
    "# If you want to transform the json data into csv, here is the instruction\n",
    "\n",
    "# First, we build a function to append data from json to csv\n",
    "# this code is adapted from Towardsdatascience blog: https://towardsdatascience.com/an-extensive-guide-to-collecting-tweets-from-twitter-api-v2-for-academic-research-using-python-3-518fcb71df2a\n",
    "\n",
    "def append_to_csv(json_response, fileName):\n",
    "\t\n",
    "\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Create headers for the data\n",
    "    csvWriter.writerow(['author id', 'created_at', 'id','conversation_id', 'like_count', 'quote_count', 'reply_count','retweet_count','source','tweet'])\n",
    "    # This only need to run at the first time. If you want to append more tweets to the same csv file, you need to comment this line of code to avoid the header being written again.\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "        \n",
    "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "        # You can change the variables based on what you want. Check the available variable from here: https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet\n",
    "   \n",
    "\n",
    "        # 1. Author ID\n",
    "        author_id = tweet['author_id']\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "        \n",
    "        # 3. Tweet ID\n",
    "        tweet_id = tweet['id']\n",
    "\n",
    "        # 4. Conversation ID\n",
    "        conversation_id = tweet['conversation_id']\n",
    "\n",
    "        # 5. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "\n",
    "        # 6. source\n",
    "        source = tweet['source']\n",
    "\n",
    "        # 7. Tweet text\n",
    "        text = tweet['text']\n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [author_id, created_at, tweet_id, conversation_id, like_count, quote_count, reply_count, retweet_count, source, text]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Tweets added from this response: \", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7f067f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import dateutil.parser\n",
    "import time\n",
    "\n",
    "# Run the belowing commented code the first time you run this code\n",
    "os.environ['TOKEN'] = 'AAAAAAAAAAAAAAAAAAAAAMfScAEAAAAALQJjppcjpBs3%2B06Sbn%2BcdAlA%2BCQ%3D52jIy4Vtch5lfjsqN2C46rwcotcVZthA6UKKJ7p24RMvHggh5y'\n",
    "\n",
    "bearer_token = os.environ.get(\"TOKEN\")\n",
    "\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2UserTweetsPython\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def create_url(start_date, end_date, max_results): # If you are retrieving recent tweets, remove \"start_date\" and \"end_date\"\n",
    "\t# crete url and set up the query parameters\n",
    "    user_id = 576287914 # you can get the user_id through Get_user.py\n",
    "   \n",
    "    search_url = \"https://api.twitter.com/2/users/{}/tweets?exclude=replies\".format(user_id) \n",
    "    # I have exluded the replies. If you want to include the replies, then you need to remove \"?exclude=replies\" from the url\n",
    "\n",
    "    #change params based on the endpoint you are using\n",
    "    query_params = {'start_time': start_date,\n",
    "                    'end_time': end_date,\n",
    "                    'max_results': max_results,\n",
    "                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function; next_token is the unique ID field for the next page of results\n",
    "    response = requests.request(\"GET\", url, auth=bearer_oauth, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def append_to_csv(json_response, fileName):\n",
    "\t# this code is adapted from Towardsdatascience blog: https://towardsdatascience.com/an-extensive-guide-to-collecting-tweets-from-twitter-api-v2-for-academic-research-using-python-3-518fcb71df2a\n",
    "\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "        \n",
    "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "        # You can change the variables based on what you want. Check the available variable from here: https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet\n",
    "   \n",
    "\n",
    "        # 1. Author ID\n",
    "        author_id = tweet['author_id']\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "        \n",
    "        # 4. Tweet ID\n",
    "        tweet_id = tweet['id']\n",
    "\n",
    "        # 5. Conversation ID\n",
    "        conversation_id = tweet['conversation_id']\n",
    "\n",
    "        # 6. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "\n",
    "        # 7. source\n",
    "        source = tweet['source']\n",
    "\n",
    "        # 8. Tweet text\n",
    "        text = tweet['text']\n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [author_id, created_at, tweet_id, conversation_id, like_count, quote_count, reply_count, retweet_count, source, text]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Tweets added from this response: \", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2cd0cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2021-10-01T00:00:00.000Z\n",
      "# of Tweets added from this response:  1\n",
      "Total # of Tweets added:  1\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2021-11-01T00:00:00.000Z\n",
      "# of Tweets added from this response:  3\n",
      "Total # of Tweets added:  4\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2021-12-01T00:00:00.000Z\n",
      "# of Tweets added from this response:  3\n",
      "Total # of Tweets added:  7\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-01-01T00:00:00.000Z\n",
      "# of Tweets added from this response:  1\n",
      "Total # of Tweets added:  8\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-02-01T00:00:00.000Z\n",
      "# of Tweets added from this response:  2\n",
      "Total # of Tweets added:  10\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2022-04-01T00:00:00.000Z\n",
      "# of Tweets added from this response:  6\n",
      "Total # of Tweets added:  16\n",
      "-------------------\n",
      "Total number of results:  16\n"
     ]
    }
   ],
   "source": [
    "# If you want to get tweets from multiple time period, here is the instruction to help you write a for loop to get the json reponse and at the time transform them to csv\n",
    "# set up the timeframe, aiming to get data from Jan 2021 till July 2021\n",
    "\n",
    "start_list =    ['2021-10-01T00:00:00.000Z',\n",
    "                 '2021-11-01T00:00:00.000Z',\n",
    "                 '2021-12-01T00:00:00.000Z',\n",
    "                 '2022-01-01T00:00:00.000Z',\n",
    "                 '2022-02-01T00:00:00.000Z',\n",
    "                 '2022-03-01T00:00:00.000Z',\n",
    "                 '2022-04-01T00:00:00.000Z',\n",
    "                 ]\n",
    "\n",
    "end_list =      ['2021-11-01T00:00:00.000Z',\n",
    "                 '2021-12-01T00:00:00.000Z',\n",
    "                 '2022-01-01T00:00:00.000Z',\n",
    "                 '2022-02-01T00:00:00.000Z',\n",
    "                 '2022-03-01T00:00:00.000Z',\n",
    "                 '2022-04-01T00:00:00.000Z',\n",
    "                 '2022-05-01T00:00:00.000Z',\n",
    "                 ]\n",
    "\n",
    "\n",
    "max_results = 100 #This defaults to 10 Tweets and has a maximum of 100. \n",
    "#Total number of tweets we collected from the loop\n",
    "total_tweets = 0\n",
    "\n",
    "\n",
    "# Create file, you need to put the file name here\n",
    "csvFile = open(\"wmtcnc.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "#Create headers for the data\n",
    "csvWriter.writerow(['author id', 'created_at', 'id','conversation_id', 'like_count', 'quote_count', 'reply_count','retweet_count','source','tweet'])\n",
    "csvFile.close()\n",
    "\n",
    "# If you are only retrieving recent tweets, you don't need the for loop, just remove it. But you need some adjustment for the codes especially the function\n",
    "\n",
    "for i in range(0,len(start_list)):\n",
    "\t# this code is adapted from Towardsdatascience blog: https://towardsdatascience.com/an-extensive-guide-to-collecting-tweets-from-twitter-api-v2-for-academic-research-using-python-3-518fcb71df2a\n",
    "\n",
    "    # Inputs\n",
    "    count = 0 # Counting tweets per time period\n",
    "    max_count = 100 # Max tweets per time period\n",
    "    flag = True\n",
    "    next_token = None\n",
    "    \n",
    "    # Check if flag is true\n",
    "    while flag:\n",
    "        # Check if max_count reached\n",
    "        if count >= max_count:\n",
    "            break\n",
    "        print(\"-------------------\")\n",
    "        print(\"Token: \", next_token)\n",
    "        url = create_url(start_list[i],end_list[i], max_results)\n",
    "        json_response = connect_to_endpoint(url[0], url[1], next_token)\n",
    "        result_count = json_response['meta']['result_count']\n",
    "\n",
    "        if 'next_token' in json_response['meta']:\n",
    "            # Save the token to use for next call\n",
    "            next_token = json_response['meta']['next_token']\n",
    "            print(\"Next Token: \", next_token)\n",
    "            if result_count is not None and result_count > 0 and next_token is not None:\n",
    "                print(\"Start Date: \", start_list[i])\n",
    "                append_to_csv(json_response, \"BMW_more_tweets.csv\") # you need to define the file name here\n",
    "                count += result_count\n",
    "                total_tweets += result_count\n",
    "                print(\"Total # of Tweets added: \", total_tweets)\n",
    "                print(\"-------------------\")\n",
    "                time.sleep(5)                \n",
    "        # If no next token exists\n",
    "        else:\n",
    "            if result_count is not None and result_count > 0:\n",
    "                print(\"-------------------\")\n",
    "                print(\"Start Date: \", start_list[i])\n",
    "                append_to_csv(json_response, \"BMW_more_tweets.csv\") # you need to define the file name here\n",
    "                count += result_count\n",
    "                total_tweets += result_count\n",
    "                print(\"Total # of Tweets added: \", total_tweets)\n",
    "                print(\"-------------------\")\n",
    "                time.sleep(5)\n",
    "            \n",
    "            #Since this is the final request, turn flag to false to move to the next time period.\n",
    "            flag = False\n",
    "            next_token = None\n",
    "        time.sleep(5)\n",
    "print(\"Total number of results: \", total_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9915f59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Response Code: 200\n",
      "{\n",
      "    \"data\": [\n",
      "        {\n",
      "            \"author_id\": \"17369233\",\n",
      "            \"conversation_id\": \"1521640246944935936\",\n",
      "            \"created_at\": \"2022-05-03T23:58:00.000Z\",\n",
      "            \"id\": \"1521640246944935936\",\n",
      "            \"lang\": \"en\",\n",
      "            \"public_metrics\": {\n",
      "                \"like_count\": 0,\n",
      "                \"quote_count\": 0,\n",
      "                \"reply_count\": 0,\n",
      "                \"retweet_count\": 0\n",
      "            },\n",
      "            \"reply_settings\": \"everyone\",\n",
      "            \"source\": \"Twitter Web App\",\n",
      "            \"text\": \"Enjoy a cup of specialty coffee made with purified water from the Pure Water Demonstration Facility at LVMWD. Join us May 14th for a morning filled with conservation, community, and COFFEE! \\u2615\\ud83d\\udca7\\n\\nRSVP at https://t.co/eqROmhbcCP\\n\\n#PureCoffee #PureWaterTastingSeries #PureWater https://t.co/hTCywl9Nb3\"\n",
      "        },\n",
      "        {\n",
      "            \"author_id\": \"862957929887956992\",\n",
      "            \"conversation_id\": \"1521639586950443009\",\n",
      "            \"created_at\": \"2022-05-03T23:55:22.000Z\",\n",
      "            \"geo\": {\n",
      "                \"place_id\": \"018cb268038d69d0\"\n",
      "            },\n",
      "            \"id\": \"1521639586950443009\",\n",
      "            \"lang\": \"en\",\n",
      "            \"public_metrics\": {\n",
      "                \"like_count\": 2,\n",
      "                \"quote_count\": 0,\n",
      "                \"reply_count\": 0,\n",
      "                \"retweet_count\": 0\n",
      "            },\n",
      "            \"reply_settings\": \"everyone\",\n",
      "            \"source\": \"Twitter for Android\",\n",
      "            \"text\": \"Coffee\\u2615\\n\\nRed Diamond Specialty Coffee\\nCentral world\\n\\nSketch \\n\\ud83d\\udd8b\\u270d\\n\\n#reddiamondspecialtycoffee #caf\\u00e9 #cofeetime\\u2615 \\n#Blackdiamondcoffeeroaster\\n#reddiamond_thailand\\n#reddiamondwatercolors https://t.co/IHHGon4nWQ\"\n",
      "        },\n",
      "        {\n",
      "            \"author_id\": \"1004465413\",\n",
      "            \"conversation_id\": \"1521628178317553666\",\n",
      "            \"created_at\": \"2022-05-03T23:10:02.000Z\",\n",
      "            \"id\": \"1521628178317553666\",\n",
      "            \"lang\": \"en\",\n",
      "            \"public_metrics\": {\n",
      "                \"like_count\": 8,\n",
      "                \"quote_count\": 0,\n",
      "                \"reply_count\": 0,\n",
      "                \"retweet_count\": 0\n",
      "            },\n",
      "            \"reply_settings\": \"everyone\",\n",
      "            \"source\": \"Sprout Social\",\n",
      "            \"text\": \"Feast your eyes on our newest specialty release, Vietnamese Iced Coffee 10W-40 Imperial Stout (8% ABV), brewed with High Noon Coffee &amp; a double dose of milk sugar to create an extra roasty &amp; creamy pastry stout. Asheville taprooms, online + full distro starting this Friday, 5/6. https://t.co/gf5zRSCcFg\"\n",
      "        },\n",
      "        {\n",
      "            \"author_id\": \"64506531\",\n",
      "            \"conversation_id\": \"1521618678139133952\",\n",
      "            \"created_at\": \"2022-05-03T22:32:17.000Z\",\n",
      "            \"id\": \"1521618678139133952\",\n",
      "            \"lang\": \"en\",\n",
      "            \"public_metrics\": {\n",
      "                \"like_count\": 0,\n",
      "                \"quote_count\": 0,\n",
      "                \"reply_count\": 0,\n",
      "                \"retweet_count\": 1\n",
      "            },\n",
      "            \"referenced_tweets\": [\n",
      "                {\n",
      "                    \"id\": \"1521065256289714176\",\n",
      "                    \"type\": \"retweeted\"\n",
      "                }\n",
      "            ],\n",
      "            \"reply_settings\": \"everyone\",\n",
      "            \"source\": \"Zoho Social\",\n",
      "            \"text\": \"RT @SpecialtyBarist: RT PerfectDailyG \\\"@RoyalCoffee is hiring!\\n\\nThe Crown: Royal Coffee Lab &amp; Tasting Room is looking for a lead barista pa\\u2026\"\n",
      "        },\n",
      "        {\n",
      "            \"author_id\": \"2699459658\",\n",
      "            \"conversation_id\": \"1521304888788938753\",\n",
      "            \"created_at\": \"2022-05-03T22:08:35.000Z\",\n",
      "            \"id\": \"1521612711209373696\",\n",
      "            \"in_reply_to_user_id\": \"1855984909\",\n",
      "            \"lang\": \"en\",\n",
      "            \"public_metrics\": {\n",
      "                \"like_count\": 4,\n",
      "                \"quote_count\": 0,\n",
      "                \"reply_count\": 2,\n",
      "                \"retweet_count\": 1\n",
      "            },\n",
      "            \"referenced_tweets\": [\n",
      "                {\n",
      "                    \"id\": \"1521606512019648512\",\n",
      "                    \"type\": \"replied_to\"\n",
      "                }\n",
      "            ],\n",
      "            \"reply_settings\": \"everyone\",\n",
      "            \"source\": \"Twitter Web App\",\n",
      "            \"text\": \"@Danwalls42 Therein lies the problem \\nIf young people today kept to the 5 x salary rule but want 14 X in an inner city suburb close to a specialty coffee shop &amp; soy lattes with hazelnut drizzle.\"\n",
      "        },\n",
      "        {\n",
      "            \"author_id\": \"1499722127867535360\",\n",
      "            \"conversation_id\": \"1521610930546782209\",\n",
      "            \"created_at\": \"2022-05-03T22:01:30.000Z\",\n",
      "            \"id\": \"1521610930546782209\",\n",
      "            \"lang\": \"en\",\n",
      "            \"public_metrics\": {\n",
      "                \"like_count\": 0,\n",
      "                \"quote_count\": 0,\n",
      "                \"reply_count\": 0,\n",
      "                \"retweet_count\": 0\n",
      "            },\n",
      "            \"reply_settings\": \"everyone\",\n",
      "            \"source\": \"WordPress.com\",\n",
      "            \"text\": \"Specialty Coffee Barista https://t.co/d9QbugJOIG\"\n",
      "        },\n",
      "        {\n",
      "            \"author_id\": \"368862762\",\n",
      "            \"conversation_id\": \"1521609986186612736\",\n",
      "            \"created_at\": \"2022-05-03T21:57:45.000Z\",\n",
      "            \"id\": \"1521609986186612736\",\n",
      "            \"lang\": \"en\",\n",
      "            \"public_metrics\": {\n",
      "                \"like_count\": 0,\n",
      "                \"quote_count\": 0,\n",
      "                \"reply_count\": 0,\n",
      "                \"retweet_count\": 22\n",
      "            },\n",
      "            \"referenced_tweets\": [\n",
      "                {\n",
      "                    \"id\": \"1521567526798385153\",\n",
      "                    \"type\": \"retweeted\"\n",
      "                }\n",
      "            ],\n",
      "            \"reply_settings\": \"everyone\",\n",
      "            \"source\": \"Twitter for Android\",\n",
      "            \"text\": \"RT @TOTMCoffee: Smooth from dawn to dusk.\\n\\n\\ud83c\\udfc6 Specialty grade coffee\\n\\ud83c\\udf0e Ethically sourced https://t.co/6W5vXLyP3m\"\n",
      "        },\n",
      "        {\n",
      "            \"author_id\": \"1000511004491513856\",\n",
      "            \"conversation_id\": \"1521609952695140353\",\n",
      "            \"created_at\": \"2022-05-03T21:57:37.000Z\",\n",
      "            \"id\": \"1521609952695140353\",\n",
      "            \"lang\": \"en\",\n",
      "            \"public_metrics\": {\n",
      "                \"like_count\": 0,\n",
      "                \"quote_count\": 0,\n",
      "                \"reply_count\": 0,\n",
      "                \"retweet_count\": 22\n",
      "            },\n",
      "            \"referenced_tweets\": [\n",
      "                {\n",
      "                    \"id\": \"1521567526798385153\",\n",
      "                    \"type\": \"retweeted\"\n",
      "                }\n",
      "            ],\n",
      "            \"reply_settings\": \"everyone\",\n",
      "            \"source\": \"Twitter for iPad\",\n",
      "            \"text\": \"RT @TOTMCoffee: Smooth from dawn to dusk.\\n\\n\\ud83c\\udfc6 Specialty grade coffee\\n\\ud83c\\udf0e Ethically sourced https://t.co/6W5vXLyP3m\"\n",
      "        },\n",
      "        {\n",
      "            \"author_id\": \"1509564114\",\n",
      "            \"conversation_id\": \"1521604899603107840\",\n",
      "            \"created_at\": \"2022-05-03T21:37:32.000Z\",\n",
      "            \"geo\": {\n",
      "                \"coordinates\": {\n",
      "                    \"coordinates\": [\n",
      "                        49.57575318,\n",
      "                        25.39898009\n",
      "                    ],\n",
      "                    \"type\": \"Point\"\n",
      "                },\n",
      "                \"place_id\": \"017189e6700ad6cd\"\n",
      "            },\n",
      "            \"id\": \"1521604899603107840\",\n",
      "            \"lang\": \"en\",\n",
      "            \"public_metrics\": {\n",
      "                \"like_count\": 0,\n",
      "                \"quote_count\": 0,\n",
      "                \"reply_count\": 0,\n",
      "                \"retweet_count\": 0\n",
      "            },\n",
      "            \"reply_settings\": \"everyone\",\n",
      "            \"source\": \"Foursquare Swarm\",\n",
      "            \"text\": \"\\ud83d\\udc9c\\ud83d\\udc9c\\ud83d\\udc9c (@ Triangle Specialty Coffee in Al Mubarraz, Ash Sharqiyah w/ @mohammed_sami70 @falthani52) https://t.co/0zDid0pGZQ\"\n",
      "        },\n",
      "        {\n",
      "            \"author_id\": \"3273360132\",\n",
      "            \"conversation_id\": \"1521589614338879488\",\n",
      "            \"created_at\": \"2022-05-03T20:36:48.000Z\",\n",
      "            \"id\": \"1521589614338879488\",\n",
      "            \"lang\": \"en\",\n",
      "            \"public_metrics\": {\n",
      "                \"like_count\": 0,\n",
      "                \"quote_count\": 0,\n",
      "                \"reply_count\": 0,\n",
      "                \"retweet_count\": 2\n",
      "            },\n",
      "            \"referenced_tweets\": [\n",
      "                {\n",
      "                    \"id\": \"1521550350423142408\",\n",
      "                    \"type\": \"retweeted\"\n",
      "                }\n",
      "            ],\n",
      "            \"reply_settings\": \"everyone\",\n",
      "            \"source\": \"Twitter for Android\",\n",
      "            \"text\": \"RT @ertheatre: Our caf\\u00e9 reopened last Saturday after more than two years!\\ud83c\\udf89\\n\\nWe're keeping it simple with coffee, specialty teas and plenty\\u2026\"\n",
      "        }\n",
      "    ],\n",
      "    \"includes\": {\n",
      "        \"places\": [\n",
      "            {\n",
      "                \"country\": \"\\u0e1b\\u0e23\\u0e30\\u0e40\\u0e17\\u0e28\\u0e44\\u0e17\\u0e22\",\n",
      "                \"country_code\": \"TH\",\n",
      "                \"full_name\": \"\\u0e1a\\u0e32\\u0e07\\u0e01\\u0e30\\u0e1b\\u0e34, \\u0e1b\\u0e23\\u0e30\\u0e40\\u0e17\\u0e28\\u0e44\\u0e17\\u0e22\",\n",
      "                \"geo\": {\n",
      "                    \"bbox\": [\n",
      "                        100.6144912,\n",
      "                        13.7628697,\n",
      "                        100.6562306,\n",
      "                        13.8193622\n",
      "                    ],\n",
      "                    \"properties\": {},\n",
      "                    \"type\": \"Feature\"\n",
      "                },\n",
      "                \"id\": \"018cb268038d69d0\",\n",
      "                \"name\": \"\\u0e1a\\u0e32\\u0e07\\u0e01\\u0e30\\u0e1b\\u0e34\",\n",
      "                \"place_type\": \"city\"\n",
      "            },\n",
      "            {\n",
      "                \"country\": \"\\u0627\\u0644\\u0645\\u0645\\u0644\\u0643\\u0629 \\u0627\\u0644\\u0639\\u0631\\u0628\\u064a\\u0629 \\u0627\\u0644\\u0633\\u0639\\u0648\\u062f\\u064a\\u0629\",\n",
      "                \"country_code\": \"SA\",\n",
      "                \"full_name\": \"\\u0627\\u0644\\u0627\\u062d\\u0633\\u0627\\u0621, \\u0627\\u0644\\u0645\\u0645\\u0644\\u0643\\u0629 \\u0627\\u0644\\u0639\\u0631\\u0628\\u064a\\u0629 \\u0627\\u0644\\u0633\\u0639\\u0648\\u062f\\u064a\\u0629\",\n",
      "                \"geo\": {\n",
      "                    \"bbox\": [\n",
      "                        49.4166539,\n",
      "                        25.2096126,\n",
      "                        49.8386985,\n",
      "                        25.7076705\n",
      "                    ],\n",
      "                    \"properties\": {},\n",
      "                    \"type\": \"Feature\"\n",
      "                },\n",
      "                \"id\": \"017189e6700ad6cd\",\n",
      "                \"name\": \"\\u0627\\u0644\\u0627\\u062d\\u0633\\u0627\\u0621\",\n",
      "                \"place_type\": \"city\"\n",
      "            }\n",
      "        ],\n",
      "        \"users\": [\n",
      "            {\n",
      "                \"created_at\": \"2008-11-13T17:16:34.000Z\",\n",
      "                \"description\": \"Providing safe, reliable drinking water and wastewater treatment services to 75,000+ customers. Call (818) 251-2100 to report a water or sewer emergency.\",\n",
      "                \"id\": \"17369233\",\n",
      "                \"name\": \"Las Virgenes MWD\",\n",
      "                \"public_metrics\": {\n",
      "                    \"followers_count\": 2276,\n",
      "                    \"following_count\": 1233,\n",
      "                    \"listed_count\": 102,\n",
      "                    \"tweet_count\": 5924\n",
      "                },\n",
      "                \"username\": \"LVMWD\",\n",
      "                \"verified\": false\n",
      "            },\n",
      "            {\n",
      "                \"created_at\": \"2017-05-12T09:09:38.000Z\",\n",
      "                \"description\": \"PHOTO & WATERCOLOR\\ud83c\\udfa8\\ud83d\\udcf7\\n\\u0e16\\u0e48\\u0e32\\u0e22\\u0e23\\u0e39\\u0e1b\\u0e14\\u0e2d\\u0e01\\u0e44\\u0e21\\u0e49\\ud83c\\udf37 \\u0e2d\\u0e32\\u0e2b\\u0e32\\u0e23\\ud83e\\uddc1 sunrise\\ud83c\\udf1e sunset\\u2600\\ufe0f\\n\\u0e40\\u0e02\\u0e35\\u0e22\\u0e19\\u0e20\\u0e32\\u0e1e\\u0e2a\\u0e35\\u0e19\\u0e49\\u0e33\\ud83c\\udfa8\\ud83d\\udd8c\\n\\u0e41\\u0e19\\u0e30\\u0e19\\u0e33\\u0e2b\\u0e19\\u0e31\\u0e07\\u0e2a\\u0e37\\u0e2d\\u0e19\\u0e48\\u0e32\\u0e2d\\u0e48\\u0e32\\u0e19\\ud83d\\udcd6\\n\\u0e2a\\u0e16\\u0e32\\u0e19\\u0e17\\u0e35\\u0e48\\u0e17\\u0e48\\u0e2d\\u0e07\\u0e40\\u0e17\\u0e35\\u0e48\\u0e22\\u0e27 \\u0e23\\u0e49\\u0e32\\u0e19\\u0e2d\\u0e32\\u0e2b\\u0e32\\u0e23 \\u0e23\\u0e49\\u0e32\\u0e19\\u0e01\\u0e32\\u0e41\\u0e1f\\n\\u0e2a\\u0e2d\\u0e19\\u0e40\\u0e02\\u0e35\\u0e22\\u0e19\\u0e23\\u0e39\\u0e1b\\u0e2a\\u0e35\\u0e19\\u0e49\\u0e33\",\n",
      "                \"id\": \"862957929887956992\",\n",
      "                \"name\": \"photo & watercolor by Vuth\",\n",
      "                \"public_metrics\": {\n",
      "                    \"followers_count\": 657,\n",
      "                    \"following_count\": 15,\n",
      "                    \"listed_count\": 8,\n",
      "                    \"tweet_count\": 4820\n",
      "                },\n",
      "                \"username\": \"vuth_photo\",\n",
      "                \"verified\": false\n",
      "            },\n",
      "            {\n",
      "                \"created_at\": \"2012-12-11T17:14:14.000Z\",\n",
      "                \"description\": \"Lager & Ales crafted in the heart of Asheville, NC with taprooms in Durham, Wilmington, Knoxville, & Louisville! Charlotte, Birmingham & Cincy (coming soon)!\",\n",
      "                \"id\": \"1004465413\",\n",
      "                \"name\": \"Hi-Wire Brewing\",\n",
      "                \"public_metrics\": {\n",
      "                    \"followers_count\": 11559,\n",
      "                    \"following_count\": 1430,\n",
      "                    \"listed_count\": 295,\n",
      "                    \"tweet_count\": 6741\n",
      "                },\n",
      "                \"username\": \"HiWireBrewing\",\n",
      "                \"verified\": false\n",
      "            },\n",
      "            {\n",
      "                \"created_at\": \"2009-08-10T20:33:34.000Z\",\n",
      "                \"description\": \"Specialty Green Coffee Importers Since 1978. We Connect Producers With Roasters and Ship Great Coffee All Over the World. #RoyalCoffeeInc #OpenSourceCoffee\",\n",
      "                \"id\": \"64506531\",\n",
      "                \"name\": \"Royal Coffee, Inc.\",\n",
      "                \"public_metrics\": {\n",
      "                    \"followers_count\": 6921,\n",
      "                    \"following_count\": 778,\n",
      "                    \"listed_count\": 196,\n",
      "                    \"tweet_count\": 2683\n",
      "                },\n",
      "                \"username\": \"RoyalCoffee\",\n",
      "                \"verified\": false\n",
      "            },\n",
      "            {\n",
      "                \"created_at\": \"2014-08-02T00:30:05.000Z\",\n",
      "                \"description\": \"Middle of the road politically which in leftie speak is far right.\\nAward winning ex-basketballer and current home roaster and coffee snob\",\n",
      "                \"id\": \"2699459658\",\n",
      "                \"name\": \"Outside Looking In\",\n",
      "                \"public_metrics\": {\n",
      "                    \"followers_count\": 4045,\n",
      "                    \"following_count\": 4996,\n",
      "                    \"listed_count\": 5,\n",
      "                    \"tweet_count\": 14928\n",
      "                },\n",
      "                \"username\": \"Connundrum54\",\n",
      "                \"verified\": false\n",
      "            },\n",
      "            {\n",
      "                \"created_at\": \"2013-09-12T01:40:48.000Z\",\n",
      "                \"description\": \"\",\n",
      "                \"id\": \"1855984909\",\n",
      "                \"name\": \"Dan\",\n",
      "                \"public_metrics\": {\n",
      "                    \"followers_count\": 43,\n",
      "                    \"following_count\": 251,\n",
      "                    \"listed_count\": 5,\n",
      "                    \"tweet_count\": 876\n",
      "                },\n",
      "                \"username\": \"Danwalls42\",\n",
      "                \"verified\": false\n",
      "            },\n",
      "            {\n",
      "                \"created_at\": \"2022-03-04T12:23:19.000Z\",\n",
      "                \"description\": \"Free Jobs Posting Website\",\n",
      "                \"id\": \"1499722127867535360\",\n",
      "                \"name\": \"CV Business\",\n",
      "                \"public_metrics\": {\n",
      "                    \"followers_count\": 14,\n",
      "                    \"following_count\": 0,\n",
      "                    \"listed_count\": 0,\n",
      "                    \"tweet_count\": 82958\n",
      "                },\n",
      "                \"username\": \"CVBusinessORG\",\n",
      "                \"verified\": false\n",
      "            },\n",
      "            {\n",
      "                \"created_at\": \"2011-09-06T10:51:28.000Z\",\n",
      "                \"description\": \"I'm a Bi artist, rusty animator, Gamer, Voice Actor, and Professional New Mom! HMU for my Discord, DA, Twitch, & Tumblr- & HOLY SHIT I'M 31?! X'D\",\n",
      "                \"id\": \"368862762\",\n",
      "                \"name\": \"ThebSayraduka\",\n",
      "                \"public_metrics\": {\n",
      "                    \"followers_count\": 62,\n",
      "                    \"following_count\": 690,\n",
      "                    \"listed_count\": 7,\n",
      "                    \"tweet_count\": 13484\n",
      "                },\n",
      "                \"username\": \"ThebSayraduka\",\n",
      "                \"verified\": false\n",
      "            },\n",
      "            {\n",
      "                \"created_at\": \"2018-05-26T22:56:26.000Z\",\n",
      "                \"description\": \"Positive Mental Attitude/ only retweets all posts from Markiplier and Jacksepticeye. All the posts from them.\",\n",
      "                \"id\": \"1000511004491513856\",\n",
      "                \"name\": \"Youtubegek\",\n",
      "                \"public_metrics\": {\n",
      "                    \"followers_count\": 15,\n",
      "                    \"following_count\": 5,\n",
      "                    \"listed_count\": 0,\n",
      "                    \"tweet_count\": 2418\n",
      "                },\n",
      "                \"username\": \"MoniqueTwigt\",\n",
      "                \"verified\": false\n",
      "            },\n",
      "            {\n",
      "                \"created_at\": \"2013-06-12T06:30:34.000Z\",\n",
      "                \"description\": \"\\u0627\\u0644\\u062d\\u0645\\u062f\\u0627\\u0644\\u0644\\u0647 \\u0623\\u0646\\u0646\\u064a \\u0623\\u0645\\u0644\\u0643 \\u0623\\u0645 \\u0639\\u0638\\u064a\\u0645\\u0629 \\u0623\\u0645 \\u0644\\u0627 \\u0645\\u062b\\u064a\\u0644 \\u0644\\u0647\\u0627 \\u2764\\ufe0f@kalalhoob2013\",\n",
      "                \"id\": \"1509564114\",\n",
      "                \"name\": \"\\u0633\\u0644\\u0645\\u0649 \\u0633\\u0627\\u0645\\u064a \\u0628\\u0648\\u0628\\u0634\\u064a\\u062a\",\n",
      "                \"public_metrics\": {\n",
      "                    \"followers_count\": 613,\n",
      "                    \"following_count\": 176,\n",
      "                    \"listed_count\": 0,\n",
      "                    \"tweet_count\": 56516\n",
      "                },\n",
      "                \"username\": \"Salma_bub91\",\n",
      "                \"verified\": false\n",
      "            },\n",
      "            {\n",
      "                \"created_at\": \"2015-07-09T21:32:37.000Z\",\n",
      "                \"description\": \"Spreading the word about UK's City of Culture 2017 & East Riding. History, music, art, film, food, travel, sport #Hull's different resonance. RT not endorsement\",\n",
      "                \"id\": \"3273360132\",\n",
      "                \"name\": \"Discover Hull\",\n",
      "                \"public_metrics\": {\n",
      "                    \"followers_count\": 6643,\n",
      "                    \"following_count\": 4412,\n",
      "                    \"listed_count\": 262,\n",
      "                    \"tweet_count\": 214886\n",
      "                },\n",
      "                \"username\": \"discoverHullEY\",\n",
      "                \"verified\": false\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"meta\": {\n",
      "        \"newest_id\": \"1521640246944935936\",\n",
      "        \"next_token\": \"b26v89c19zqg8o3fpywl7umyk5kzxobcx7cf384tdzf5p\",\n",
      "        \"oldest_id\": \"1521589614338879488\",\n",
      "        \"result_count\": 10\n",
      "    }\n",
      "}\n",
      "# of Tweets added from this response:  10\n"
     ]
    }
   ],
   "source": [
    "# The folowing code is used to help search tweets with keyword\n",
    "\n",
    "# The following codes for connecting to Twitter API to get full conversation are adapted from the sample codes given by Twitterdev through GitHub repository: https://github.com/twitterdev/Twitter-API-v2-sample-code/tree/main/Recent-Search\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "# Run the belowing commented code the first time\n",
    "os.environ['TOKEN'] = 'AAAAAAAAAAAAAAAAAAAAAMfScAEAAAAALQJjppcjpBs3%2B06Sbn%2BcdAlA%2BCQ%3D52jIy4Vtch5lfjsqN2C46rwcotcVZthA6UKKJ7p24RMvHggh5y'\n",
    "\n",
    "bearer_token = os.environ.get(\"TOKEN\")\n",
    "\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2UserTweetsPython\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def create_url(keyword, start_time, end_time, max_results): # If you are retrieving recent tweets, remove \"start_date\" and \"end_date\" \n",
    "    \n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/recent\" # collect recent tweets (last 7 days)\n",
    "\n",
    "    # For understanding of the fields available, please check: https://developer.twitter.com/en/docs/twitter-api/fields\n",
    "    query_params = {'start_time':start_time,\n",
    "                    'end_time': end_time,\n",
    "                    'query': keyword,\n",
    "                    'max_results': max_results,\n",
    "                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url,params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, auth=bearer_oauth, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "# you don't need to define the time, if you only can search for recent tweets\n",
    "max_results = 10\n",
    "keyword = 'specialty coffee'\n",
    "# here is the official documentation to help you understand how to build query with correct operators: https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query\n",
    "\n",
    "start_time = '2022-04-28T00:00:00.000Z'\n",
    "end_time = '2022-05-04T00:00:00.000Z'\n",
    "# Remember with standard account, you can only retrieve recent 7-day tweets with search_tweet endpoint, so when you are setting the timeframe the earliest date you can set is 7 day before your current date\n",
    "url = create_url(keyword, start_time, end_time, max_results)\n",
    "json_response = connect_to_endpoint(url[0], url[1])\n",
    "\n",
    "print(json.dumps(json_response, indent=4, sort_keys=True))\n",
    "\n",
    "\n",
    "\n",
    "# If you want to transform the json data into csv, here is the instruction\n",
    "\n",
    "# First, we build a function to append data from json to csv\n",
    "# this code is adapted from Towardsdatascience blog: https://towardsdatascience.com/an-extensive-guide-to-collecting-tweets-from-twitter-api-v2-for-academic-research-using-python-3-518fcb71df2a\n",
    "\n",
    "def append_to_csv(json_response, fileName):\n",
    "    \n",
    "\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Create headers for the data\n",
    "    csvWriter.writerow(['author id', 'created_at', 'id','conversation_id', 'like_count', 'quote_count', 'reply_count','retweet_count','source','tweet'])\n",
    "    # This only need to run at the first time. If you want to append more tweets to the same csv file, you need to comment this line of code to avoid the header being written again.\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "        \n",
    "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "        # You can change the variables based on what you want. Check the available variable from here: https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet\n",
    "   \n",
    "\n",
    "        # 1. Author ID\n",
    "        author_id = tweet['author_id']\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "        \n",
    "        # 3. Tweet ID\n",
    "        tweet_id = tweet['id']\n",
    "\n",
    "        # 4. Conversation ID\n",
    "        conversation_id = tweet['conversation_id']\n",
    "\n",
    "        # 5. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "\n",
    "        # 6. source\n",
    "        source = tweet['source']\n",
    "\n",
    "        # 7. Tweet text\n",
    "        text = tweet['text']\n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [author_id, created_at, tweet_id, conversation_id, like_count, quote_count, reply_count, retweet_count, source, text]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Tweets added from this response: \", counter)\n",
    "\n",
    "# run the function with the json response and set up the file name\n",
    "\n",
    "append_to_csv(json_response, 'tweets_whitehouse.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1966018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1521640246944935936\n",
      "conversation_id:1521640246944935936\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 400\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "(400, '{\"errors\":[{\"parameters\":{\"start_time\":[\"2022-04-27T00:00Z\"]},\"message\":\"Invalid \\'start_time\\':\\'2022-04-27T00:00Z\\'. \\'start_time\\' must be on or after 2022-04-27T15:21Z\"}],\"title\":\"Invalid Request\",\"detail\":\"One or more parameters to your request was invalid.\",\"type\":\"https://api.twitter.com/2/problems/invalid-request\"}')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d051cc87bcff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_results\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# we create for loops to search every conversation ID and get the replies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# If you are retrieving recent tweets, remove \"start_time\" and \"end_time\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mjson_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnect_to_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mresult_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_response\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'meta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-d051cc87bcff>\u001b[0m in \u001b[0;36mconnect_to_endpoint\u001b[0;34m(url, params, next_token)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Endpoint Response Code: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: (400, '{\"errors\":[{\"parameters\":{\"start_time\":[\"2022-04-27T00:00Z\"]},\"message\":\"Invalid \\'start_time\\':\\'2022-04-27T00:00Z\\'. \\'start_time\\' must be on or after 2022-04-27T15:21Z\"}],\"title\":\"Invalid Request\",\"detail\":\"One or more parameters to your request was invalid.\",\"type\":\"https://api.twitter.com/2/problems/invalid-request\"}')"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import dateutil.parser\n",
    "import time\n",
    "\n",
    "# Basically we got the conversation IDs of all the tweets through user timeline search and next we are creating a list which contains all the conversation IDs and this list will act as keyword list as params when connecting to the API endpoints\n",
    "df = pd.read_csv(\"tweets_whitehouse.csv\") # put the name of the tweet file that you have retrieved\n",
    "replies_list = df.conversation_id.tolist()\n",
    "print(replies_list[0])\n",
    "\n",
    "# Create all the conversion_ids as keywords and put into a list\n",
    "keyword_list = []\n",
    "for k in replies_list:\n",
    "    a = \"conversation_id:\"+str(k)\n",
    "    keyword_list.append(a)\n",
    "\n",
    "print(keyword_list[0])\n",
    "\n",
    "\n",
    "# The following codes for connecting to Twitter API to get full conversation are adapted from the sample codes given by Twitterdev through GitHub repository: https://github.com/twitterdev/Twitter-API-v2-sample-code/tree/main/Recent-Search\n",
    "\n",
    "\n",
    "# Run the belowing commented code the first time\n",
    "os.environ['TOKEN'] = 'AAAAAAAAAAAAAAAAAAAAAMfScAEAAAAALQJjppcjpBs3%2B06Sbn%2BcdAlA%2BCQ%3D52jIy4Vtch5lfjsqN2C46rwcotcVZthA6UKKJ7p24RMvHggh5y'\n",
    "\n",
    "bearer_token = os.environ.get(\"TOKEN\")\n",
    "\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2UserTweetsPython\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def create_url(keyword, start_date, end_date, max_results): # If you are retrieving recent tweets, remove \"start_date\" and \"end_date\"\n",
    "    \n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/recent\" # collect recent tweets (last 7 days)\n",
    "\n",
    "    # For understanding of the fields available, please check: https://developer.twitter.com/en/docs/twitter-api/fields\n",
    "    query_params = {'query': keyword,\n",
    "                    'start_time': start_date, \n",
    "                    'end_time': end_date,\n",
    "                    'max_results': max_results,\n",
    "                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url,params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, auth=bearer_oauth, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "def append_to_csv(json_response, fileName):\n",
    "    # this code is adapted from Towardsdatascience blog: https://towardsdatascience.com/an-extensive-guide-to-collecting-tweets-from-twitter-api-v2-for-academic-research-using-python-3-518fcb71df2a\n",
    "    \n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "        \n",
    "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "        # You can change the variables based on what you want. Check the available variable from here: https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet\n",
    "\n",
    "        # 1. Author ID\n",
    "        author_id = tweet['author_id']\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "        # 4. Tweet ID\n",
    "        tweet_id = tweet['id']\n",
    "\n",
    "        # 5. Conversation ID\n",
    "        conversation_id = tweet['conversation_id']\n",
    "\n",
    "        # 6. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "\n",
    "        # 7. source\n",
    "        source = tweet['source']\n",
    "\n",
    "        # 8. Tweet text\n",
    "        text = tweet['text']\n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [author_id, created_at, tweet_id, conversation_id, like_count, quote_count, reply_count, retweet_count, source, text]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Tweets added from this response: \", counter) \n",
    "\n",
    "# you don't need to define the time, if you only can search for recent tweets\n",
    "start_time = \"2022-04-27T00:00:00.000Z\"\n",
    "end_time = \"2022-05-04T00:00:00.000Z\"\n",
    "max_results = 100\n",
    "\n",
    "#Total number of tweets we collected from the loop\n",
    "total_tweets = 0\n",
    "\n",
    "\n",
    "# Create file, you need to put the file name here\n",
    "csvFile = open(\"File_name.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "#Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n",
    "csvWriter.writerow(['author id', 'created_at', 'id','conversation_id', 'like_count', 'quote_count', 'reply_count','retweet_count','source','tweet'])\n",
    "csvFile.close()\n",
    "\n",
    "for i in range(0,len(keyword_list)): # create for loop for your conversation_ids\n",
    "    # this code is adapted from Towardsdatascience blog: https://towardsdatascience.com/an-extensive-guide-to-collecting-tweets-from-twitter-api-v2-for-academic-research-using-python-3-518fcb71df2a\n",
    "\n",
    "    # Inputs\n",
    "    count = 0 # Counting tweets per time period\n",
    "    max_count = 100 # Max tweets per time period, you can adjust it to higher number\n",
    "    flag = True\n",
    "    next_token = None\n",
    "    \n",
    "    # Check if flag is true\n",
    "    while flag:\n",
    "        # Check if max_count reached\n",
    "        if count >= max_count:\n",
    "            break\n",
    "        print(\"-------------------\")\n",
    "        print(\"Token: \", next_token)\n",
    "        url = create_url(keyword_list[i],start_time, end_time, max_results) # we create for loops to search every conversation ID and get the replies\n",
    "        # If you are retrieving recent tweets, remove \"start_time\" and \"end_time\"\n",
    "        json_response = connect_to_endpoint(url[0], url[1], next_token)\n",
    "        result_count = json_response['meta']['result_count']\n",
    "\n",
    "        if 'next_token' in json_response['meta']:\n",
    "            # Save the token to use for next call\n",
    "            next_token = json_response['meta']['next_token']\n",
    "            print(\"Next Token: \", next_token)\n",
    "            if result_count is not None and result_count > 0 and next_token is not None:\n",
    "                print(\"conversation_id: \", keyword_list[i])\n",
    "                append_to_csv(json_response, \"File_name.csv\")\n",
    "                count += result_count\n",
    "                total_tweets += result_count\n",
    "                print(\"Total # of Tweets added: \", total_tweets)\n",
    "                print(\"-------------------\")\n",
    "                time.sleep(5)                \n",
    "        # If no next token exists\n",
    "        else:\n",
    "            if result_count is not None and result_count > 0:\n",
    "                print(\"-------------------\")\n",
    "                print(\"conversation_id: \", keyword_list[i])\n",
    "                append_to_csv(json_response, \"File_name.csv\")\n",
    "                count += result_count\n",
    "                total_tweets += result_count\n",
    "                print(\"Total # of Tweets added: \", total_tweets)\n",
    "                print(\"-------------------\")\n",
    "                time.sleep(5)\n",
    "            \n",
    "            #Since this is the final request, turn flag to false to move to the next time period.\n",
    "            flag = False\n",
    "            next_token = None\n",
    "        time.sleep(5)\n",
    "print(\"Total number of results: \", total_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5285fc0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
